\section*{Problem 3}
We have used in this problem a similar architecture for the VAE's decoder and the GAN's generator which is an MLP with 6 layers, as shown in this code snippet:
\lstinputlisting[language=Python]{gangen.py}

We have decided to go with this architecture, after having tested several architectures for both models, including convolutionnal neural network. We noticed that the VAE model works fine with a convolutional architecture whereas the GAN have not got good result with that kind of architecture. Both models have been trained with a latent variable dimension of 100.

\begin{itemize}
\item[A.] {\textbf{Qualitative Evaluations}}\\
\begin{enumerate}
	\item[1.]{Visual samples}\\
	We have generated different samples from both models (Figures~\ref{fig:q3vae} and ~\ref{fig:q3gan}). We notice that the images generated by the VAE seem realistic albeit blurry, whereas the images generated by GAN are more diversified, and seem less realistic. 
	%
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{Q3vaesample.png}
		\caption{Samples generated with VAE}
		\label{fig:q3vae}
	\end{figure}
	
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.4\linewidth}
			\includegraphics[scale=1]{Q3gansample.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\linewidth}
			\includegraphics[scale=1]{Q3gansample2.png}
		\end{subfigure}
		\caption{Samples generated with GAN.}
		\label{fig:q3gan}
	\end{figure}

\item[2.]{Learning the disentangled representation in the latent space.}
Figures \ref{fig:des1} and \ref{fig:desVAE} show how the models learned a disentangled representation. Starting from one point in the latent space we apply a perturbation to two arbitrary dimensions. We can observe how in both models the resulting images vary smoothly. The value of perturbation ($\epsilon$) is however different for each model, as they require different levels of perturbation to result in visible changes.
\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{gandesent1.png}
	\quad
	\includegraphics[scale=1]{gandesent2.png}
	\caption{Samples by perturbating the dimensions, left: 6 and 96, right: 25 and 75}
	\label{fig:des1}
\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=1]{gandesent2.png}
%	\caption{Samples by perturbating dimensions 25 and 75}
%	\label{fig:des2}
%\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{sample_VAE21-65_epsilon_90.png}
	\quad
	\includegraphics[scale=1]{sample_VAE21-8_epsilon_60.png}
	\caption{Samples by perturbating the dimensions, left: 65 and 90, right: 8 and 60}
	\label{fig:desVAE}
\end{figure}

\item[3.] {Interpolation in the data space and in the latent space:}
Figures \ref{fig:vae_latent}, \ref{fig:vae_image}, \ref{fig:gan_latent} and\ref{fig:gan_image} show the interpolations in the latent and image space for VAE and GAN respectively. For both models we can observe how the interpolation in the latent space produce images that have more \emph{meaning} compared to the interpolation in the image space, where the transition images are only superposition of the patterns (numbers) in each of the initial images. This can be explained by the latent space forming a kind of \emph{manifold} along which the interpolation makes more sense than in the pixel space. 

\begin{enumerate}
	\item VAE:\\
%	Moving along $\alpha$, we observe a smooth transition from one sample to the other.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{vae_latent.png}
		\caption{Interpolation in the latent space using VAE}
		\label{fig:vae_latent}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{vae_image.png}
		\caption{Interpolation in the data space using VAE}
		\label{fig:vae_image}
	\end{figure}

	\item GAN:\\ 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{GAN_latent.png}
		\caption{Interpolation in the latent space using GAN}
		\label{fig:gan_latent}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{GAN_image.png}
		\caption{Interpolation in the data space using GAN}
		\label{fig:gan_image}
	\end{figure}
	

	\end{enumerate}


\end{enumerate}

    \item [B.] {\textbf{Quantitative Evaluations}}\\
    \begin{enumerate}
        \item[1.] 
       We have used the provided functions to extract the representations of the images. We compute the Frechet Inception Distance by estimating the mean and covariance of the generator's/decoder's distribution. The calculation steps are explained in the following code snippet:
       \lstinputlisting[language=Python]{fid.py}
       
        \item[2.] We sampled 1000 images from each generative models and calculate the FID-score as instructed. The results are: 
        \begin{itemize}
        	\item For the GAN, the FID score is: $29 526.37$
        	\item For the VAE, the FID score is: $51 355.12$
        \end{itemize}
        This metric confirme our ascertainment that the GAN is more realistic than the VAE, given the ground truth given by the provided classifier.
        
    \end{enumerate}
\end{itemize}

