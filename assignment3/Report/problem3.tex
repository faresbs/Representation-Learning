\section*{Problem 3}
We have used in this problem a similare architecture for the VAE's decoder and the GAN's generator which is an MLP with 6 layers, as shown in this code snippet:
\lstinputlisting[language=Python]{gangen.py}

We have decided to go with this architecture, after having tested several architectures for both models, including convolutionnal neural network. We noticed that the VAE model works fine with a convolutional architecture whereas the GAN have not got good result with that kind of architecture.

\begin{itemize}
\item[A.] {\textbf{Qualitative Evaluations}}\\
\begin{enumerate}
	\item[1.]{Visual samples}
	We have generated differents samples from both models as we can see below in Figures~\ref{fig:q3vae} and ~\ref{fig:q3gan}. We notice that the images generated by the VAE model are very clear but a little blurry, whereas the images generated by the GAN model are more deversified, and seem more realistic. 
	%
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{Q3vaesample.png}
		\caption{Samples generated with VAE}
		\label{fig:q3vae}
	\end{figure}
	
	\textbf{@FARRIS PUT HERE SOME SAMPLES}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.4\linewidth}
			\includegraphics[scale=1]{Q3gansample.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\linewidth}
			\includegraphics[scale=1]{Q3gansample2.png}
		\end{subfigure}
		\caption{Samples generated with GAN.}
		\label{fig:q3gan}
	\end{figure}

\item[2.]{Learning the disentangled representation in the latent space}
The following figures show how the GAN has learned a desentagled representation:
\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{gandesent1.png}
	\caption{Samples by perturbating dimensions 6 and 96}
	\label{fig:des1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{gandesent2.png}
	\caption{Samples by perturbating dimensions 25 and 75}
	\label{fig:des2}
\end{figure}

\textbf{TO BE CONTINUED BY FARRIS FOR VAE}

\item[3.] {Interpolation in the data space and in the latent space:}

\textbf{TBD BY JORGE FOR GAN AND FARRIS FOR VAE}



\end{enumerate}

    \item [B.] {\textbf{Quantitative Evaluations}}\\
    \begin{enumerate}
        \item[1.] 
       We have used the provided functions to extract the representations of the images. We compute the Frechet Inception Distance by estimating the mean and covariance of the generator's/decoder's distribution. The calculation steps are explained in the following code snippet:
       \lstinputlisting[language=Python]{fid.py}
       
        \item[2.] We sampled 1000 images from each generative models and calculate the FID-score as instructed. The results are: 
        \begin{itemize}
        	\item For the GAN, the FID score is: $29 526.37$
        	\item For the VAE, the FID score is: $51 355.12$
        \end{itemize}
        This metric confirme our ascertainment that the GAN is more realistic than the VAE, given the ground truth given by the provided classifier.
        
    \end{enumerate}
\end{itemize}

