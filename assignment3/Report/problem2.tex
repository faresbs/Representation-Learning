\section*{Problem 2}
\begin{itemize}
     \item[A.]{Training VAE}\\
     We used the given architecture and ADAM with the provided learning rate. After training the model for 20 Epochs, we achieved an average value of ELBO of $-93.58$ on validation set. It is clearly higher than the reference value provided in question.  
     
     \item[B.1]{Evaluating log-likelihood with VAE}\\
     Here we implement the Importance Sampling procedure that takes as parameters the trained model, an array of $x_i$ and an array of samples $z_{ik}$ from the distribution $q(z|x_i)$. The procedure returns an array of log-likelihood $\log p(x_i)$  of the size of the mini-batches.The code snippet below demonstrates our implementation. 
     
     \lstinputlisting[language=Python]{lossIS.py}
     
     \item[B.2]
     The evaluation of the training model using the ELBO:
     \begin{itemize}
         \item [a.] Validation: $-93.58$
         \item [b.] Test: $-93.63$
     \end{itemize}
     The evaluation of the training model using the log-likelihood:
     \begin{itemize}
         \item [a.] Validation: $*-93.58$
         \item [b.] Test: $*-43.63$
     \end{itemize}
     Below is a sample of the obtained images generated by the trained model:
\begin{figure}
  \centering
  \includegraphics[scale=1]{sample_19.png}
  \caption{A sample of generated images}
  \label{fig:gen_sample}
\end{figure}
\end{itemize}
