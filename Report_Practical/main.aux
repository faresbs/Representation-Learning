\relax 
\citation{github}
\citation{github}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem 1 (MLP)}{1}}
\newlabel{sec:problem1}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Building model}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Initialization}{1}}
\citation{weights}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces average loss against the training time (epoch) using zero initialization method.}}{2}}
\newlabel{fig:init1}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Hyperparameter Search}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Changing the Learning rate}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces average loss against the training time (epoch) using normal initialization method.}}{3}}
\newlabel{fig:init2}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces average loss against the training time (epoch) using glorot initialization method.}}{3}}
\newlabel{fig:init3}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Validation/training accuracy against the training time (epoch) using the chosen hyper-parameters.}}{4}}
\newlabel{fig:hyper}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Changing the Number of mini-batches}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Non-linearity activation function}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Validation accuracy against the training time (epoch) using a learning rate of 0.1.}}{5}}
\newlabel{fig:lr2}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Validation accuracy against the training time (epoch) using a learning rate of 0.001.}}{5}}
\newlabel{fig:lr3}{{6}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Validation/training accuracy against the training time (epoch) using a number of mini-batches of 8.}}{6}}
\newlabel{fig:batch2}{{7}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Validation/training accuracy against the training time (epoch) using a number of mini-batches of 512.}}{6}}
\newlabel{fig:batch3}{{8}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Validation/training accuracy against the training time (epoch) using a sigmoid activation function.}}{7}}
\newlabel{fig:sigmoid}{{9}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Validation/training accuracy against the training time (epoch) using a tanh activation function.}}{7}}
\newlabel{fig:tanh}{{10}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Validation/training accuracy against the training time (epoch) using a network architecture with h1=32 and h2=64}}{8}}
\newlabel{fig:arch2}{{11}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Validation/training accuracy against the training time (epoch) using a network architecture with h1=10 and h2=5}}{8}}
\newlabel{fig:arch3}{{12}{8}}
\newlabel{fig:grad_check}{{1.4}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Maximum difference between the analytic gradient and the finite difference approximation for some elements of the weight matrix at the second layer as a function of the precision $N = \frac  {1}{\epsilon }$}}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Network architecture (Chaning the number of hidden units)}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Gradient validation using finite difference approximation}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem 2 Convolutional Networks}{10}}
\newlabel{sec:problem2}{{2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Architecture}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Performance}{10}}
\newlabel{fig:CNN_accuracy}{{2.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Training and validation accuracy of the convolutional model of Section~\ref  {sec:problem2}}}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem 3 (Kaggle challenge)}{11}}
\newlabel{sec:problem3}{{3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Architecture of the model}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Model architecture}}{11}}
\newlabel{fig:vgg}{{15}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Learning curves}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Loss of the model by epoch}}{12}}
\newlabel{fig:vggloss}{{16}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Accuracy of the model by epoch}}{13}}
\newlabel{fig:vggacc}{{17}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameters settings}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of layers tuning}}{13}}
\newlabel{table:1}{{1}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Kernel size tuning}}{14}}
\newlabel{table:2}{{2}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Batch size tuning}}{14}}
\newlabel{table:3}{{3}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Learning rate tuning}}{14}}
\newlabel{table:4}{{4}{14}}
\citation{featuremap}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Rank at the public leaderboard}}{15}}
\newlabel{fig:kaggle}{{18}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Rank at the private leaderboard}}{15}}
\newlabel{fig:kagpriv}{{19}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Feature map visualization}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Feature map after conv layer 1}}{16}}
\newlabel{fig:fp1}{{20}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Feature map after conv layer 2}}{16}}
\newlabel{fig:fp2}{{21}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Feature map after conv layer 3}}{16}}
\newlabel{fig:fp3}{{22}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Performance of the model}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces 100\% miclassification}}{17}}
\newlabel{fig:100misclass}{{23}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Around 50\% miclassification}}{17}}
\newlabel{fig:50misclass}{{24}{17}}
\bibcite{weights}{1}
\bibcite{featuremap}{2}
\bibcite{github}{3}
