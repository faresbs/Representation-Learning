% (meta)
% Exercise contributed by Julian Zaidi
% Translation by Salem Lahlou
% label: ch6

\Exercise{
\label{ex:activation_transform}
On considère un réseau de neurones à deux couches $y:\R^D\rightarrow\R^K$ de la forme suivante:
$$
y(x, \Theta, \sigma)_k=\sum_{j=1}^M \omega_{kj}^{(2)}\sigma\left(\sum_{i=1}^D \omega_{ji}^{(1)}x_i+\omega_{j0}^{(1)}\right)+\omega_{k0}^{(2)}
$$
pour $1\leq k\leq K$. Les paramètres du réseau sont $\Theta=(\omega^{(1)},\omega^{(2)})$. La fonction d'activation utilisée est $\sigma$, la fonction logistique. 
Montrez qu'il existe un réseau équivalent de la même forme, avec des paramètres  $\Theta'=(\tilde{\omega}^{(1)},\tilde{\omega}^{(2)})$, avec la fonction d'activation $\tanh$, tel que $y(x, \Theta', \tanh)=y(x, \Theta, \sigma)$ pour tout $x \in \R^D$. Exprimez $\Theta'$ en fonction de $\Theta$.
}

\Answer{
${}$%placeholder
}
