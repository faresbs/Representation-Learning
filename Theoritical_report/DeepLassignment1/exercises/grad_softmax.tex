% (meta) 
% Exercise contributed by Louis-Guillaume Gagnon
% label: ch6

\Exercise{
\label{ex:grad_softmax}
Let $x$ be an $n$-dimensional vector. 
Recall the softmax function: $S: \vx \in \R^n \mapsto S(\vx) \in \R^n$ such that $S(\vx)_i=\frac{e^{\vx_i}}{\sum_j e^{\vx_j}}$; 
the diagonal function: $\text{diag}(\vx)_{ij}=\vx_i$ if $i=j$ and $\text{diag}(\vx)_{ij}=0$ if $i\neq j$;
and the Kronecker delta function: $\delta_{ij}=1$ if $i=j$ and $\delta_{ij}=0$ if $i\neq j$.  

\begin{enumerate}
\item Show that the derivative of the softmax function is $\frac{d S(\vx)_i}{d \vx_j}=S(\vx)_i\left(\delta_{ij}-S(\vx)_j\right)$.

\item Express the Jacobian matrix $\frac{\partial S(\vx)}{\partial \vx}$ using matrix-vector notation. 
Use $\text{diag}(\cdot)$.

\item Compute the Jacobian of the sigmoid function $\sigma(\vx) = 1/(1 + e^{-\vx})$.

\item Let $\vy$ and $\vx$ be $n-$dimensional vectors related by $\vy = f(\vx)$, $L$ be an unspecified differentiable loss function.
According to the chain rule of calculus, $ \nabla_\vx L = (\frac{\partial \vy}{\partial \vx})^\top  \nabla_\vy L$, which takes up $\gO(n^2)$ computational time in general. 
Show that if $f(\vx)=\sigma(\vx)$ or $f(\vx)=S(\vx)$, the above matrix-vector multiplication can be simplified to a $\gO(n)$ operation.
\end{enumerate}
}

\Answer{
Q 2.1 We first use the quotient rule, and then simplify: 
$$\frac{d S(\vx)_i}{d \vx_j}=
\frac{d}{d \vx_j}\frac{e^{\vx_i}}{\sum_k e^{\vx_k}}
=
\frac{\delta_{ij}e^{\vx_i}\sum_k e^{\vx_k}-(\sum_k \delta_{kj}e^{\vx_k})
\frac{e^{\vx_i}}{\sum_k e^{\vx_k}} }{(\sum_k e^{\vx_k})^2}
=\frac{\delta_{ij}e^{\vx_i}-(\sum_k \delta_{kj}e^{\vx_k})
\frac{e^{\vx_i}}{\sum_k e^{\vx_k}} }{\sum_k e^{\vx_k}}
$$
$$
\frac{d S(\vx)_i}{d \vx_j}=
\frac{\delta_{ij}e^{\vx_i}-(\sum_k \delta_{kj}e^{\vx_k})
\frac{e^{\vx_i}}{\sum_k e^{\vx_k}} }{\sum_k e^{\vx_k}}
=
\delta_{ij}
\frac{e^{\vx_i}}
{\sum_k e^{\vx_k}}
-
\frac{\sum_k \delta_{kj}e^{\vx_k}}
{\sum_k e^{\vx_k}}
\frac{e^{\vx_i}}{\sum_k e^{\vx_k}}
=\delta_{ij}S(\vx)_i
-S(\vx)_i S(\vx)_j
$$
Q 2.2: Since the entries of the Jacobian $J$ are of the form $J_{ij}=S(\vx)_i(\delta_{ij}-S(\vx)_j)$, it is easily seen that the Jacobian is symmetric. Its diagonal elements are of the form $J_{ii}=
=S(\vx)_i(1-(S(\vx)_i)$ and its non-diagonal elements are of the form $J_{ij}=-S(\vx)_iS(\vx)_j$\\
$$J=\begin{bmatrix}
    J_{11} & J_{12} & J_{13} & \dots  & J_{1n} \\
    J_{21} & J_{22} & J_{23} & \dots  & J_{2n} \\
    J_{31} & J_{32} & J_{33} & \dots  & J_{3n}\\ 
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    J_{n1} & J_{n2} & J_{n3} & \dots  & J_{nn}
\end{bmatrix}
=
\begin{bmatrix}
    S(\vx)_1(1-(S(\vx)_1) & -S(\vx)_1S(\vx)_2 & -S(\vx)_1S(\vx)_3 & \dots  \\
    -S(\vx)_1S(\vx)_2 & S(\vx)_2(1-(S(\vx)_2) &  -S(\vx)_2S(\vx)_3& \dots  \\
    -S(\vx)_1S(\vx)_3 & -S(\vx)_2S(\vx)_3 & S(\vx)_3(1-(S(\vx)_3) & \dots  \\ 
    \vdots & \vdots & \vdots & \ddots 
\end{bmatrix}
$$
This can be rewritten as
$$
J=\begin{bmatrix}
    S(\vx)_1 & 0 & 0 & \dots  \\
    0 & S(\vx)_2 &  0& \dots  \\
    0 & 0 & S(\vx)_3 & \dots  \\ 
    \vdots & \vdots & \vdots & \ddots 
\end{bmatrix}
-\begin{bmatrix}
    S(\vx)_1S(\vx)_1 & S(\vx)_1S(\vx)_2 & S(\vx)_1S(\vx)_3 & \dots  \\
    S(\vx)_1S(\vx)_2 & S(\vx)_2S(\vx)_2) &  S(\vx)_2S(\vx)_3& \dots  \\
    S(\vx)_1S(\vx)_3 & S(\vx)_2S(\vx)_3 & S(\vx)_3S(\vx)_3 & \dots  \\ 
    \vdots & \vdots & \vdots & \ddots 
\end{bmatrix}
$$
or 
$$
J=diag(S(\vx)_i)-S(\vx)S(\vx)^T\;\;\text{where we define $S(\vx)^T=[S(\vx)_1,S(\vx)_2,S(\vx)_3,\dots]$}
$$
Q 2.3: The definition of a vector valued $\sigma(\vx)$ does not seem correct but we assume that it means
$$\sigma(\vx)=\begin{bmatrix}
    \sigma(\vx)_1 \\
    \sigma(\vx)_2\\
    \sigma(\vx)_3\\ 
    \vdots  \\
     \sigma(\vx)_n
\end{bmatrix}
=\begin{bmatrix}
    1/(1 + e^{-x_1})\\
    1/(1 + e^{-x_2})\\
    1/(1 + e^{-x_3})\\ 
    \vdots  \\
     1/(1 + e^{-x_n})
\end{bmatrix}$$
Since for a scalar $x$ the derivative   $\frac{d\sigma(x)}{dx}=\sigma(x)(1-\sigma(x))$, the Jacobian of $\sigma$ is
$$
J_\sigma=\begin{bmatrix}
    \sigma(\vx)_1 (1-\sigma(\vx)_1)& 0 & 0 & \dots  \\
    0 & \sigma(\vx)_2 (1-\sigma(\vx)_2)&  0& \dots  \\
    0 & 0 & \sigma(\vx)_3 (1-\sigma(\vx)_3)& \dots  \\ 
    \vdots & \vdots & \vdots & \ddots 
\end{bmatrix}=diag(\sigma(\vx)_i (1-\sigma(\vx)_i))
$$
Q 2.4: $ \nabla_\vx L = (\frac{\partial \vy}{\partial \vx})^\top  \nabla_\vy L$ is the product of the transpose of the ($n\times n$) Jacobian matrix $(\frac{\partial \vy}{\partial \vx})^\top$ by the $n\times 1$ dimensional column vector $\nabla_\vy L$. When $f(\vx)=\sigma(\vx)$ the Jacobian is a diagonal matrix with $n$ diagonal entries, hence its product with $\nabla_\vy L$ requires $n$ multiplications, one multiplication per each diagonal entry: $\gO(n)$. \\
When $f(\vx)=S(\vx)$ the Jacobian can be written in two parts. The first part is a diagonal matrix with $n$ diagonal entries. The product of this part of the Jacobian with the column vector $\nabla_\vy L$ is $\gO(n)$. The second part of the Jacobian...  
}