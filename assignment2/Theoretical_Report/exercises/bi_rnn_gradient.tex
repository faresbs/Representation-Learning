% (meta)
% Exercise contributed by Gabriele Prato
% label: ch10 

\Exercise{
\label{ex:bi_rnn_gradient}
Denote by $\sigma$ the logistic sigmoid function.
Consider the following Bidirectional RNN:
\begin{align*}
\vh^{(f)}_t & = \sigma(\mW^{(f)} \vx_t + \mU^{(f)} \vh^{(f)}_{t-1})\\
\vh^{(b)}_t & = \sigma(\mW^{(b)} \vx_t + \mU^{(b)} 
\vh^{(b)}_{t+1})\\
\vy_t & = \mV^{(f)} \vh^{(f)}_t + \mV^{(b)} \vh^{(b)}_t
\end{align*}
where the superscripts $f$ and $b$ correspond to the forward and backward RNNs respectively. 
\begin{enumerate}
\item Draw the computational graph for this RNN, unrolled for 3 time steps (from $t=1$ to $t=3$) Include and label the initial hidden states for both the forward and backward RNNs, $h_{0}^{(f)}$ and $h_{4}^{(b)}$ respectively. You may draw this by hand; you may also use a computer rendering package such as TikZ, but you are not required to do so.
Label each node and edge with the corresponding hidden unit or weight. 
\staritem Let $\vz_t$ be the true target of the prediction $\vy_t$ and consider the sum of squared loss $L=\sum_t L_t$ where $L_t=||\vz_t - \vy_t||_2^2$. 
Express the gradients $\nabla_{\vh_{t}^{(f)}}L$ and $\nabla_{\vh_{t}^{(b)}}L$ recursively
(in terms of $\nabla_{\vh_{t+1}^{(f)}}L$ and $\nabla_{\vh_{t-1}^{(b)}}L$ respectively).
Then derive $\nabla_{\mW^{(f)}}L$ and $\nabla_{\mU^{(b)}}L$.

\end{enumerate}
}

\Answer{
${}$%placeholder
}
