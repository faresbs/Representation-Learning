% (meta)
% Exercise contributed by Julian Zaidi
% label: ch6

\Exercise{
\label{ex:activation_transform}
Consider a 2-layer neural network $y:\R^D\rightarrow\R^K$ of the form:
$$
y(x, \Theta, \sigma)_k=\sum_{j=1}^M \omega_{kj}^{(2)}\sigma\left(\sum_{i=1}^D \omega_{ji}^{(1)}x_i+\omega_{j0}^{(1)}\right)+\omega_{k0}^{(2)}
$$
for $1\leq k\leq K$, with parameters $\Theta=(\omega^{(1)},\omega^{(2)})$ and logistic sigmoid activation function $\sigma$.
Show that there exists an equivalent network of the same form, with parameters $\Theta'=(\tilde{\omega}^{(1)},\tilde{\omega}^{(2)})$ and $\tanh$ activation function, such that $y(x, \Theta', \tanh)=y(x, \Theta, \sigma)$ for all $x\in\R^D$, and express $\Theta'$ as a function of $\Theta$.
}

\Answer{
${}$%placeholder
Write your answer here.
}
