% (meta)
% Exercise contributed by chinwei
% label: ch10

\Exercise{
\label{ex:rnn_spectral}
This question is about activation functions and vanishing/exploding gradients in recurrent neural networks (RNNs). Let $\sigma:\R\rightarrow\R$ be an activation function. 
When the argument is a vector, we apply $\sigma$ element-wise. 
Consider the following recurrent unit:
$$\vh_t = \mW \sigma(\vh_{t-1}) + \mU\vx_t + \vb$$
\begin{enumerate}
\item Show that applying the activation function in this way is equivalent to the conventional way of applying the activation function: $\vg_t = \sigma(\mW\vg_{t-1} + \mU\vx_t + \vb)$ (i.e. express $\vg_t$ in terms of $\vh_t$).
\staritem Let $||\mA||$ denote the $L_2$ operator norm
\footnote{
The $L_2$ operator norm of a matrix $\mA$ is is an \textit{induced norm} corresponding to the $L_2$ norm of vectors. 
You can try to prove the given properties as an exercise.
}
of matrix $\mA$ ($||\mA||:=\max_{\vx:||\vx||=1}||\mA\vx||$). 
Assume $\sigma(x)$ has bounded derivative, i.e. $|\sigma'(x)|\leq \gamma$ for some $\gamma>0$ and for all $x$. We denote as $\lambda_1(\cdot)$ the largest eigenvalue of a symmetric matrix. Show that if the largest eigenvalue of the weights is upper-bounded by $\frac{\delta^2}{\gamma^2}$ for some $0 \leq \delta < 1$, gradients of the hidden state will vanish over time,    
 i.e.  
$$\lambda_1(\mW^\top\mW)\leq\frac{\delta^2}{\gamma^2} \quad\implies\quad \bigg{|}\bigg{|}\frac{\partial\vh_T}{\partial\vh_0}\bigg{|}\bigg{|}  \rightarrow0 \,\text{ as }\,T\rightarrow\infty$$

Use the following properties of the $L_2$ operator norm 
$$||\mA\mB||\leq||\mA||\,||\mB|| \qquad \text{ and } \qquad ||\mA||=\sqrt{\lambda_1(\mA^\top\mA)}$$

\item What do you think will happen to the gradients of the hidden state if the condition in the previous question is reversed, i.e. if the largest eigenvalue of the weights is larger than $\frac{\delta^2}{\gamma^2}$? 
Is this condition \textit{necessary} or \textit{sufficient} for the gradient to explode? (Answer in 1-2 sentences).
\end{enumerate}
}

\Answer{
${}$%placeholder
}

