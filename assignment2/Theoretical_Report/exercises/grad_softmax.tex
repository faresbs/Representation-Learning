% (meta) 
% Exercise contributed by Louis-Guillaume Gagnon
% label: ch6

\Exercise{
\label{ex:grad_softmax}
Let $x$ be an $n$-dimensional vector. 
Recall the softmax function: $S: \vx \in \R^n \mapsto S(\vx) \in \R^n$ such that $S(\vx)_i=\frac{e^{\vx_i}}{\sum_j e^{\vx_j}}$; 
the diagonal function: $\text{diag}(\vx)_{ij}=\vx_i$ if $i=j$ and $\text{diag}(\vx)_{ij}=0$ if $i\neq j$;
and the Kronecker delta function: $\delta_{ij}=1$ if $i=j$ and $\delta_{ij}=0$ if $i\neq j$.  

\begin{enumerate}
\item Show that the derivative of the softmax function is $\frac{d S(\vx)_i}{d \vx_j}=S(\vx)_i\left(\delta_{ij}-S(\vx)_j\right)$.

\item Express the Jacobian matrix $\frac{\partial S(\vx)}{\partial \vx}$ using matrix-vector notation. 
Use $\text{diag}(\cdot)$.

\item Compute the Jacobian of the sigmoid function $\sigma(\vx) = 1/(1 + e^{-\vx})$.

\item Let $\vy$ and $\vx$ be $n-$dimensional vectors related by $\vy = f(\vx)$, $L$ be an unspecified differentiable loss function.
According to the chain rule of calculus, $ \nabla_\vx L = (\frac{\partial \vy}{\partial \vx})^\top  \nabla_\vy L$, which takes up $\gO(n^2)$ computational time in general. 
Show that if $f(\vx)=\sigma(\vx)$ or $f(\vx)=S(\vx)$, the above matrix-vector multiplication can be simplified to a $\gO(n)$ operation.
\end{enumerate}
}

\Answer{
${}$%placeholder
Write your answer here.
}