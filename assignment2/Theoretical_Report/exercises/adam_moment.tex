% (meta)
% Exercise contributed by Wesley Chung
% label: ch8 

\Exercise{
\label{ex:adam_moment}
In this question you will demonstrate that an estimate of the first moment of the gradient using an (exponential) running average is equivalent to using momentum, and is biased by a scaling factor. The goal of this question is for you to consider the relationship between different optimization schemes, and to practice noting and quantifying the effect (particularly in terms of bias/variance) of \textit{estimating} a quantity.

Let $\vg_t$ be an unbiased sample of gradient at time step $t$ and $\Delta\vtheta_t$ be the update to be made.
Initialize $\vv_0$ to be a vector of zeros.
\begin{enumerate}
\item For $t\geq1$, consider the following update rules:
    \begin{itemize}
      \item SGD with momentum:
        $$ \vv_t = \alpha \vv_{t-1} + \epsilon \vg_t \qquad \Delta\vtheta_t = -\vv_t $$
        where $\epsilon>0$ and $\alpha\in(0,1)$.
      \item SGD with running average of $\vg_t$:
        $$ \vv_t = \beta \vv_{t-1} + (1-\beta) \vg_t \qquad \Delta\vtheta_t = -\delta \vv_t $$
        where $\beta\in(0,1)$ and $\delta>0$. 
    \end{itemize} 
Express the two update rules recursively ($\Delta\vtheta_t$ as a function of $\Delta\vtheta_{t-1}$). 
Show that these two update rules are equivalent; i.e. express $(\alpha,\epsilon)$ as a function of $(\beta,\delta)$.

\item Unroll the running average update rule, i.e. express $\vv_t$ as a linear combination of $\vg_i$'s ($1\leq i\leq t$).

\item Assume $\vg_t$ has a stationary distribution independent of $t$. 
Show that the running average is biased, i.e. $\E[\vv_t]\neq\E[\vg_t]$. Propose a way to eliminate such a bias by rescaling $\vv_t$. 
  
\end{enumerate}}

\Answer{
${}$%placeholder
\begin{enumerate}
	\item 
	\begin{itemize}
		\item For SGD with momentum:
		\begin{equation} \label{eq11}
		\begin{split}
		\Delta\vtheta_t & =  -\vv_t\\
		& = -\alpha \vv_{t-1} -  \epsilon \vg_t\\
		& = \Delta\vtheta_{t-1} - \epsilon \vg_t\\
		\end{split}
		\end{equation}
		
		\item For SGD with running average:
		\begin{equation}\label{eq12}
		\begin{split}
		\Delta\vtheta_t & = -\delta \vv_t\\
		& = -\delta \beta \vv_{t-1} -\delta (1-\beta) \vg_t \\
		& = \beta \Delta\vtheta_{t-1} -\delta (1-\beta) \vg_t\\
		\end{split}
		\end{equation}
		
		\item The two equations are equivalent if we consider:
		$$\alpha = \beta\;\; and \quad \epsilon = \delta (1 - \beta)$$
	\end{itemize}

	\item From equation~\ref{eq12} we have 
	\begin{equation}
	\begin{split}
		 \vv_t & = \beta \vv_{t-1} + (1-\beta) \vg_t \\
		 & = \beta (\beta \vv_{t-2} + (1-\beta) \vg_{t-1}) + (1-\beta) \vg_t \\
		 & = \beta^2 \vv_{t-2} + \beta (1-\beta) \vg_{t-1} + (1-\beta) \vg_t \\
		 & = \beta^2 (\beta \vv_{t-3} + (1-\beta) \vg_{t-2}) + \beta (1-\beta) \vg_{t-1} + (1-\beta) \vg_t \\
		 & = \beta^3 \vv_{t-3} + \beta^2 (1-\beta) \vg_{t-2} + \beta (1-\beta) \vg_{t-1} + (1-\beta) \vg_t \\
		 \vdots\\
		 & = \beta^t \vv_0 + \sum_{i=1}^{t}\beta^{t-i}(1-\beta)\vg_i \\
	\end{split}
	\end{equation}
	Given $\vv_0 = 0$, we finaly get:
	\begin{equation}\label{eq14}
	 \vv_t = (1-\beta)\sum_{i=1}^{t}\beta^{t-i}\vg_i
	\end{equation}
	
	\item Using equation~\ref{eq14}, we get:
	$$\E(\vv_t) = (1-\beta)\sum_{i=1}^{t}\beta^{t-i}\E(\vg_i)$$
	
	If we assume $\vg_t$ has a stationary distribution for a sufficiently large $t$, we get:
	$$\E(\vv_t) = \E(\vg_t)(1-\beta)\sum_{i=1}^{t}\beta^{t-i}$$
	
	On other hand, we have:
	\begin{equation}
	\begin{split}
	(1-\beta)\sum_{i=1}^{t}\beta^{t-i} & = (1-\beta)(\beta^{t-1}+\beta^{t-2}+\cdots+\beta^2+\beta+1)\\
		& = \beta^{t-1}+\beta^{t-2}+\cdots+\beta^2+\beta+1 - \beta^{t}-\beta^{t-1}-\cdots-\beta^2-\beta\\
		& = 1 - \beta^{t}\\
	\end{split}
	\end{equation}
	
	Finaly we get:
	$$\E(\vv_t) = (1 - \beta^{t})\E(\vg_t)$$
	Which is a biased estimate as $\E(\vv_t) \neq \E(\vg_t)$.\\
	Instead, if we want an unbaised estimate we can rescale $\vv_t$ by $\frac{1}{1-\beta^{t}}$
	
	
	\end{enumerate}
}

