% (meta)
% Exercise contributed by Sherjil Ozair
% label: ch8

\Exercise{
\label{ex:glorot_forward}
The goal of this question is for you to understand the reasoning behind different parameter initializations for deep networks, particularly to think about the ways that the initialization affects the activations (and therefore the gradients) of the network. Consider the following equation for the $t$-th layer of a deep network:
$$\vh^{(t)}=g(\va^{(t)}) \qquad\qquad \va^{(t)}=\mW^{(t)}\vh^{(t-1)}+\vb^{(t)}$$
where $\va^{(t)}$ are the preactivations and $\vh^{(t)}$ are the activations for layer $t$, $g$ is an activation function, $\mW^{(t)}$ is a $d^{(t)}\times d^{(t-1)}$ matrix, and $\vb^{(t)}$ is a $d^{(t)} \times 1$ bias vector. 
The bias is initialized as a constant vector $\vb^{(t)}=[c,..,c]^\top$ for some $c\in\R$, and the entries of the weight matrix are initialized by sampling i.i.d. from either (a) a Gaussian distribution $\mW_{ij}^{(t)}\sim\gN(\mu,\sigma^2)$, or (b) a Uniform distribution $\mW_{ij}^{(t)}\sim U(\alpha,\beta)$.

For both of the assumptions (1 and 2) about the distribution of the inputs to layer $t$ listed below, and for both (a) Gaussian, and (b) Uniform sampling, design an initialization scheme that would achieve preactivations with zero-mean and unit variance at layer $t$, i.e.:  $\E[\va^{(t)}_i]=0$ and $\Var(\va^{(t)}_i)=1$, for $1\leq i\leq d^{(t)}$. 

(Hint: if $X\perp Y$,
$\Var(XY)=\Var(X)\Var(Y)+\textnormal{Var}(X)\E[Y]^2+\Var(Y)\E[X]^2$)

\begin{enumerate}
\item  Assume $\E[\vh^{(t-1)}_i]=0$ and $\Var(\vh^{(t-1)}_i)=1$ for $1\leq i\leq d^{(t-1)}$. 
Assume entries of $\vh^{(t-1)}$ are uncorrelated (the answer should not depend on $g$). 
\begin{enumerate}
    \item Gaussian: give values for $c$, $\mu$, and $\sigma^2$ as a function of $d^{(t-1)}$.
    \item Uniform: give values for $c$, $\alpha$, and $\beta$  as a function of $d^{(t-1)}$.
\end{enumerate}

\item Assume that the preactivations of the previous layer satisfy $\E[\va^{(t-1)}_i]=0$, $\Var(\va^{(t-1)}_i)=1$ and $\va_i^{(t-1)}$ has a symmetric distribution for $1\leq i\leq d^{(t-1)}$.
Assume entries of $\va^{(t-1)}$ are uncorrelated. 
Consider the case of ReLU activation: $g(x)=\max\{0,x\}$. 
\begin{enumerate}
    \item Gaussian: give values for $c$, $\mu$, and $\sigma^2$ as a function of $d^{(t-1)}$.
    \item Uniform: give values for $c$, $\alpha$, and $\beta$ as a function of $d^{(t-1)}$.
    \item What popular initialization scheme has this form?
    \item Why do you think this initialization would work well in practice? Answer in 1-2 sentences. %Think about the form of the ReLU activation function, and why we would want preactivations to be centered around 0. Why also would we want the variance of preactivations to be 1? 
\end{enumerate}
\end{enumerate}

}

\Answer{
${}$%placeholder
}
